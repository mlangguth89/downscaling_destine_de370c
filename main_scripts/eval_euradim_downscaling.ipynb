{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d146322c-8a6d-4c3b-bf19-6501d7337be7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Postprocessing EURADIM downscaling models\n",
    "\n",
    "### Background\n",
    "In scope of the Destiantion Earth Use Case DE370c, two WGANs for downscaling $NO_x$ and $O_3$ have been trained. \n",
    "For this, archived 5 km EURAD-IM forecasts between 2012 and 2018 have been used. Although the performance of the downscaling models is unsatisfactory due to a lack of informative predictor information in the archived forecast data, the following Jupyter Notebook allows for a statistical evaluation of the results. \n",
    "The deduced results have been used in the deliverables\n",
    "- D370c.5.5.1 \n",
    "- D370c.7.2.1\n",
    "\n",
    "We start by importing the necessary Python-packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a237d398-2fa2-4109-9f25-27d1dae020e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, sys, glob\n",
    "base_dir = os.path.join(os.getcwd(), \"..\")\n",
    "dirs2path = [\"handle_data\", \"postprocess\", \"utils\"]\n",
    "for dir2path in dirs2path:\n",
    "    sys.path.append(os.path.join(base_dir, dir2path))\n",
    "import json as js\n",
    "from datetime import datetime as dt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import matplotlib as mpl\n",
    "import cartopy.crs as ccrs\n",
    "from handle_data_class import prepare_dataset\n",
    "from all_normalizations import ZScore\n",
    "from statistical_evaluation import Scores\n",
    "from postprocess import run_evaluation_time, run_evaluation_spatial\n",
    "from plotting import plot_downscaling_examples\n",
    "from other_utils import convert_to_xarray, get_lcc_coords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751c5795-5881-4a1e-b4ab-725610bafc21",
   "metadata": {},
   "source": [
    "### Base directories for test dataset and model\n",
    "\n",
    "The data and model directory are determined with a couple of parameters as follows:\n",
    " - `task`: Two options are avialble: 'O3' and 'NOx' \n",
    " - `data_dir`: directory where the test dataset is stored\n",
    " - `model_base_dir`: top-level directory where trained downscaling models are saved\n",
    " - `output_base_dir`: output directory for results/plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2923fbd-caed-4f77-be6e-2584bb7dda40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_type = \"sha_wgan\"\n",
    "\n",
    "# custom parameters\n",
    "task = \"O3\"                           #  \"NOx\"\n",
    "data_dir = f\"/p/scratch/deepacf/maelstrom/maelstrom_data/ap5/downscaling_destine_aq/euradim_downscaling/{task.lower()}\"\n",
    "model_base_dir = os.path.join(base_dir, \"trained_models\", \"destine_final\")\n",
    "output_base_dir = os.path.join(base_dir, \"results\")\n",
    "eps = 0.01                                                       # epsilon parametre for log-transformed data\n",
    "\n",
    "# derived parameters\n",
    "expname = f\"{model_type}_{task.lower()}\"          \n",
    "downscaling_dataset = f\"euradim_{task.lower()}\"\n",
    "\n",
    "model_base = os.path.join(model_base_dir, expname)\n",
    "norm_dir, model_dir = model_base, os.path.join(model_base, f\"{expname}_generator\")\n",
    "plt_dir = os.path.join(output_base_dir, expname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973dedb3-ed94-4d4d-bd59-2552a4165bbd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load configuration and trained model\n",
    "\n",
    "Read configuration files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551a7802-6e5f-4e82-ad78-e92bb4675375",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create output directory\n",
    "os.makedirs(plt_dir, exist_ok=True)\n",
    "\n",
    "# read configuration files\n",
    "md_config_pattern, ds_config_pattern = f\"config_{model_type}.json\", f\"config_ds_{downscaling_dataset}.json\"\n",
    "md_config_file, ds_config_file = glob.glob(os.path.join(model_base, md_config_pattern)), \\\n",
    "                                 glob.glob(os.path.join(model_base, ds_config_pattern))\n",
    "if not ds_config_file:\n",
    "    raise FileNotFoundError(f\"Could not find expected configuration file for dataset '{ds_config_pattern}' \" +\n",
    "                            f\"under '{model_base}'\")\n",
    "else:\n",
    "    with open(ds_config_file[0]) as dsf:\n",
    "        print(f\"Read dataset configuration file '{ds_config_file[0]}'.\")\n",
    "        ds_dict = js.load(dsf)\n",
    "\n",
    "if not md_config_file:\n",
    "    raise FileNotFoundError(f\"Could not find expected configuration file for model '{md_config_pattern}' \" +\n",
    "                            f\"under '{model_base}'\")\n",
    "else:\n",
    "    with open(md_config_file[0]) as mdf:\n",
    "        print(f\"Read model configuration file '{md_config_file[0]}'.\")\n",
    "        hparams_dict = js.load(mdf)\n",
    "\n",
    "named_targets = hparams_dict.get(\"named_targets\", False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aab82cd-64e3-433a-b820-04df043574ca",
   "metadata": {},
   "source": [
    "Next, load the trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30305241-ec27-4bfe-8593-9aa5e841a923",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get trained model\n",
    "trained_model = keras.models.load_model(model_dir, compile=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927d15f8-2eaa-4de4-8827-796c6d7e6bbc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prepare dataset \n",
    "Prepare the dataset for inference, get the ground truth data and figure out how to process the output (log-transformation, residual approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952950ac-3c6b-4c0d-a318-ddcbd308597a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get normalization parameters\n",
    "js_norm = os.path.join(norm_dir, \"norm.json\")\n",
    "print(\"Read normalization file for subsequent data transformation.\")\n",
    "data_norm = ZScore(ds_dict[\"norm_dims\"])\n",
    "data_norm.read_norm_from_file(js_norm)\n",
    "\n",
    "# get dataset pipeline for inference\n",
    "tfds_opts = {\"batch_size\": ds_dict[\"batch_size\"], \"predictands\": ds_dict[\"predictands\"], \"predictors\": ds_dict[\"predictors\"],\n",
    "             \"lshuffle\": False, \"var_tar2in\": ds_dict.get(\"var_tar2in\", None), \"named_targets\": named_targets, \"lrepeat\": False, \"drop_remainder\": False}\n",
    "\n",
    "tfds_test, test_info = prepare_dataset(data_dir, downscaling_dataset, ds_dict, hparams_dict, \"val\", tfds_opts[\"predictands\"], \n",
    "                                       norm_obj=data_norm, norm_dims=None, shuffle=tfds_opts[\"lshuffle\"], lrepeat=tfds_opts[\"lrepeat\"],\n",
    "                                       drop_remainder=tfds_opts[\"drop_remainder\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bbe8fd-69c8-4433-9194-ce45b0f8a349",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get ground truth data\n",
    "fval = test_info.get(\"file\")\n",
    "tar_varname = test_info[\"varnames_tar\"][0]\n",
    "\n",
    "ds_test = xr.open_dataset(fval)\n",
    "ground_truth = ds_test[tar_varname.replace(\"ln\", \"\").replace(\"_res\", \"\")].astype(\"float32\", copy=True)\n",
    "\n",
    "# get predictors and predictands\n",
    "predictors = ds_dict.get(\"predictors\", None)\n",
    "if predictors is None:\n",
    "    predictors = [var for var in list(ds_test.data_vars) if var.endswith(\"_in\")]\n",
    "\n",
    "tfds_opts[\"predictors\"] = predictors\n",
    "tfds_opts[\"predictands\"] = test_info[\"varnames_tar\"]\n",
    "\n",
    "# retrieve parameters to handle inference data\n",
    "varname_base = tar_varname.split(\"_\")[0]\n",
    "varname_in = f\"{varname_base}_in\"\n",
    "\n",
    "if \"_res\" in tar_varname:\n",
    "    lresidual_app = True\n",
    "    \n",
    "    y_corr = ds_test[varname_in]\n",
    "else: \n",
    "    lresidual_app = False\n",
    "    \n",
    "llog = False\n",
    "if f\"ln\" in tar_varname: llog = True\n",
    "\n",
    "print(f\"Target data: {tar_varname} \\nInput data: {varname_in} \\nGround truth data: {tar_varname.replace('ln', '').replace('_res', '')} is ground truth.\")\n",
    "print(f\"Residual approach? {lresidual_app}\")\n",
    "print(f\"Log transformation? {llog}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a627c77c-e948-4461-b22a-6dd98ce22641",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0561d5f6-2c5c-45ef-b19e-d83238f8b91a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# start inference\n",
    "print(\"Start inference on trained model...\")\n",
    "#with tf.device('/CPU:0'):\n",
    "y_pred = trained_model.predict(tfds_test, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97687a60-502a-4b7c-ad77-ffb737d96238",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Postprocess\n",
    "\n",
    "Turn raw inference data into xr.DataArray and un-do log-transformation and residual approach if required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721ebd17-cb76-45f6-9f9d-1681a576a7bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = convert_to_xarray(y_pred, data_norm, tar_varname, ground_truth.coords, ground_truth.dims)\n",
    "if lresidual_app: y_pred = y_pred + y_corr\n",
    "if llog: y_pred = eps*np.exp(y_pred)-eps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f3f9e6-8b4b-48ef-ba22-2b5ef4fb1db3",
   "metadata": {
    "tags": []
   },
   "source": [
    "Write downscaling results to netCDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8826611a-8e76-48c7-837b-1711e0c64bbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ncfile_out = os.path.join(plt_dir, f\"downscaled_{expname}.nc\")\n",
    "\n",
    "print(f\"Write inference data to netCDF-file '{ncfile_out}'\")\n",
    "ground_truth.name, y_pred.name = f\"{tar_varname}_ref\", f\"{tar_varname}_fcst\"\n",
    "ds = xr.Dataset(xr.Dataset.merge(y_pred.to_dataset(), ground_truth.to_dataset()))\n",
    "ds.to_netcdf(ncfile_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d218ba68-08b6-4450-8238-10b217ebee7f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Statistical evaluation\n",
    "\n",
    "The statistical evaluation is performed in terms of the following metrics:\n",
    "- RMSE\n",
    "- Bias\n",
    "\n",
    "These metrics are evaluated by averaging over all dimensions of the test dataset (time, y, x) and by averaging over the time.\n",
    "The former provides insight into the overall performance of the downscaling model, whereas the latter allows for an assessment on the spatial dependency of the performance (spatial evaluation). <br><br>\n",
    "We start with the overall evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd8cd78-a493-4b08-9b59-98ea4a049bfb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# start evaluation\n",
    "print(f\"Output data on test dataset successfully processed. Start overall evaluation...\")\n",
    "\n",
    "# instantiate score engine for time evaluation (i.e. hourly time series of evalutaion metrics)\n",
    "score_engine = Scores(y_pred, ground_truth, ds_dict[\"norm_dims\"][1:])\n",
    "\n",
    "# run evaluation\n",
    "rmse_all = run_evaluation_time(score_engine, \"rmse\", \"ppbv\", plt_dir, value_range=(0., 10.), model_type=model_type, varname=task)\n",
    "bias_all = run_evaluation_time(score_engine, \"bias\", \"ppbv\", plt_dir, value_range=(-1., 1.), ref_line=0.,\n",
    "                               model_type=model_type, varname=task)\n",
    "\n",
    "print(f\"RMSE on test year: {rmse_all.mean().values: .2f} (+-{rmse_all.std().values: .2f}) ppbV\")\n",
    "print(f\"Bias on test year: {bias_all.mean().values: .2f} (+-{bias_all.std().values: .2f}) ppbV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb806316-aa82-41bb-8cb4-cfbc0d6fc847",
   "metadata": {
    "tags": []
   },
   "source": [
    "Start spatial evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81733d13-95f7-4e67-a8ec-2e8b4248a0d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PROJ-parameters corresponding to the 1x1 km**2 target domain\n",
    "proj_params = {'central_latitude': 54.,\n",
    "               'central_longitude': 12.5,\n",
    "               'standard_parallels': (30., 60),\n",
    "               \"false_easting\": 450499.7817318188,\n",
    "               \"false_northing\": 357999.42235067615}\n",
    "\n",
    "# Create a Lambert Conformal Conic projection object\n",
    "proj = ccrs.LambertConformal(**proj_params)\n",
    "proj_params[\"earth_radius\"] = 6370000.\n",
    "\n",
    "# Do spatial evaluation\n",
    "score_engine = Scores(y_pred, ground_truth, [])\n",
    "\n",
    "lvl_rmse = np.array([0., 0.25, .5, 1.] + list(np.arange(2., 11.1, 1)))\n",
    "cmap_rmse = mpl.cm.afmhot_r(np.linspace(0., 1., len(lvl_rmse)))\n",
    "_ = run_evaluation_spatial(score_engine, \"rmse\", os.path.join(plt_dir, \"rmse_spatial\"), \n",
    "                           dims=ds_dict[\"norm_dims\"][1::], cmap=cmap_rmse, levels=lvl_rmse,\n",
    "                           projection=proj, extent=[5.5, 8., 50.5, 52.])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5de01e1-4a87-4851-ae36-5c61ec4cdeee",
   "metadata": {
    "tags": []
   },
   "source": [
    "Comparison against the bilinearly interpolated input data serving as simple baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76115476-c5a2-4ba0-8730-dd9d09bbeecf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# baseline bilinear interpolation\n",
    "score_engine = Scores(ds_test[varname_in.replace(\"ln\", \"\").replace(\"_res\", \"\")], ground_truth, ds_dict[\"norm_dims\"][1:])\n",
    "\n",
    "rmse = score_engine(\"rmse\")\n",
    "bias = score_engine(\"bias\") \n",
    "\n",
    "print(f\"RMSE on test year: {rmse.mean().values: .2f} (+-{rmse.std().values: .2f}) ppbV\")\n",
    "print(f\"Bias on test year: {bias.mean().values: .2f} (+-{bias.std().values: .2f}) ppbV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c914c9fc-4855-4715-bf67-27eb2e51a2fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Plot examples \n",
    "\n",
    "With the following, specific examples from the test dataset can be plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8521441b-998e-4c33-bc42-d69bec6e1cbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# auxiliary variables\n",
    "lon, lat = get_lcc_coords(ds_test[\"x\"], ds_test[\"y\"], proj_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425588d8-923e-4ca6-9c32-932887b057ef",
   "metadata": {
    "tags": []
   },
   "source": [
    "Different time step indices can be selected. The rest will be handled automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a7c4bc-e70d-468d-99a8-37cdbd842950",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# custom parameter\n",
    "tstep = 4800 \n",
    "levels = np.arange(0., 40.)\n",
    "\n",
    "# get target filename and datetime of sample\n",
    "tnow = pd.to_datetime(y_pred.isel({'time': tstep})['time'].values)\n",
    "fname = os.path.join(plt_dir, f\"downscaling_{model_type}_{tnow.strftime('%Y%m%d_%H00')}.png\")\n",
    "\n",
    "plot_downscaling_examples(ds_test[varname_in.replace(\"ln\", \"\")].isel({\"time\": tstep}).values, ground_truth.isel({\"time\": tstep}).values,\n",
    "                         y_pred.isel({\"time\": tstep}).values, lon, lat, fname, varname=\"NOx\", levels=levels, proj_data=proj)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langguth1_downscaling_kernel_juwels",
   "language": "python",
   "name": "langguth1_downscaling_kernel_juwels"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
